{"cells":[{"metadata":{"_uuid":"4a33e396d43e595c4d98bfeb76321a43ae81a058"},"cell_type":"markdown","source":"I wrote this kernel to mess around with the sketch data and share my experience so far.  After exploring the website and the data, I'll use a very basic CNN to classify sketches. This model gets 0.60 on the Public LB when run with 6000 recognized images per class.\n\n## Quick, Draw\nOk - I have to say this is kind of a fun thing. You go to the [Quick Draw](https://quickdraw.withgoogle.com/#)  website and agree to sketch several common objects. The host then gives you the object labels one by one, and you have 20 seconds to draw each one. If the AI guesses your sketch (that is, associates it with training set items of the same label) you get a check mark and move on. At the end you get something like this:\n\n<img src=\"https://s3.amazonaws.com/nonwebstorage/Screenshot+from+2018-09-26+22-44-21.png\" alt=\"drawing1\" width=\"600\"/>\n  \n<p><br></p>\nNotice that it didn't like my bird. Apparently it looked more like a dragon or a diving board(??) or a mosquito.  They're nice enough to show you how other people draw birds so you maybe learn how to draw better.\n<img src=\"https://s3.amazonaws.com/nonwebstorage/Screenshot+from+2018-09-26+22-45-21.png\" alt=\"drawing1\" width=\"600\"/>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"## Training Images\nWe can look at a few sketches and then see what the training data contains overall. I'll use an adaptation of Inversion's 'Getting Started' kernel."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import os\nimport re\nfrom glob import glob\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nimport ast\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"376d2827a7f11e80f6591bfc439a9d5558ef10fc"},"cell_type":"markdown","source":"Here's what the training data looks like. This data frame is actually a concatenation of two rows from each of 6 csv files in the training set."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"5090c701f9121207b67e13a0ae3f239514dff217","scrolled":true},"cell_type":"code","source":"fnames = glob('../input/train_simplified/*.csv')\ncnames = ['countrycode', 'drawing', 'key_id', 'recognized', 'timestamp', 'word']\ndrawlist = []\nfor f in fnames[0:6]:\n    first = pd.read_csv(f, nrows=10) # make sure we get a recognized drawing\n    first = first[first.recognized==True].head(2)\n    drawlist.append(first)\ndraw_df = pd.DataFrame(np.concatenate(drawlist), columns=cnames)\ndraw_df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"60f35cc05283cc4f4871cec37bc873f5c9706042"},"cell_type":"markdown","source":"And here are some nice sketches..."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"2be57621533d0e0aa75108febde01261d2444c37"},"cell_type":"code","source":"evens = range(0,11,2)\nodds = range(1,12, 2)\ndf1 = draw_df[draw_df.index.isin(evens)]\ndf2 = draw_df[draw_df.index.isin(odds)]\n\nexample1s = [ast.literal_eval(pts) for pts in df1.drawing.values]\nexample2s = [ast.literal_eval(pts) for pts in df2.drawing.values]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"scrolled":false,"_uuid":"0a1aaca9129ad3748e597993c90ab65c5fdefd5b"},"cell_type":"code","source":"labels = df2.word.tolist()\nfor i, example in enumerate(example1s):\n    plt.figure(figsize=(6,3))\n    \n    for x,y in example:\n        plt.subplot(1,2,1)\n        plt.plot(x, y, marker='.')\n        plt.axis('off')\n\n    for x,y, in example2s[i]:\n        plt.subplot(1,2,2)\n        plt.plot(x, y, marker='.')\n        plt.axis('off')\n        label = labels[i]\n        plt.title(label, fontsize=10)\n\n    plt.show()  ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cf8bc6e13cba236c6632cff33d9729d6eff728f4"},"cell_type":"markdown","source":"## The Quick, Draw Data Repository\nThere is a direct data source outside of Kaggle that seems pretty useful. The main link is a [GitHub Repository](https://github.com/googlecreativelab/quickdraw-dataset) that leads to the data in several formats, including Numpy bitmap files. Each file in the dataset covers a specific type of sketch and is in the shape of a long 1d array. Here are a few.\n\nUpdate: The test set for this project doesn't have premade files in the numpy bitmap format. When you apply your model to test arrays converted from matplotlib, there is a loss due to the different conversion process. The best results for an image-based model are had by using the same processing for train and test (no surprise there I guess). So I'd say these are good for exploration but consider the other files for better results."},{"metadata":{"trusted":true,"_uuid":"73e317f188154d2b908f75ee8aeafc06da86db42","scrolled":true,"_kg_hide-input":true},"cell_type":"code","source":"# # commenting this out to save memory\n# import urllib\n\n# data_dir = './'\n# model_dir = './'\n# LABELS = np.array(['baseball', 'bowtie', 'clock', 'hand', 'hat'])\n# num_classes = len(LABELS)\n# ims_per_class = 10\n# for b in tqdm(LABELS):\n#     url = \"https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/{}.npy\".format(b)\n#     urllib.request.urlretrieve(url, \"{}.npy\".format(b))\n# print(glob('./*.npy'))\n# nb = np.load(\"{}.npy\".format(b))\n# print(\"\\n Class '{0}' has {1} examples of {2}x{2} images\".format(b, nb.shape[0], int(nb.shape[1]**0.5)))\n# del nb","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8b9f5b85771add0d7672f7611a62e4668a28453c"},"cell_type":"markdown","source":"## Convolutional Neural Network (CNN)\nNext we'll build an image classifier. There are some resources in the repository I mentioned earlier showing how people have used the data. One of those resources is a CNN just like what you see here. \n\nThe biggest usage of resources seems to be converting the drawings to images. You can stick with a stroke-based model or go down the conversion path. Going that way requires watching data usage and managing space limits - deep learning on Kaggle can be like building a ship in a bottle:) "},{"metadata":{"trusted":true,"_uuid":"779113b4b0ef5fbe50ddda59813b019b3b769b58"},"cell_type":"code","source":"#%% import\nimport os\nfrom glob import glob\nimport re\nimport ast\nimport numpy as np \nimport pandas as pd\nfrom PIL import Image, ImageDraw \nimport urllib\nfrom tqdm import tqdm\nfrom dask import bag\nfrom dask.diagnostics import ProgressBar\n# ProgressBar().register()\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential, model_from_json\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\n\n#%% set global vars and params\nclass_files = os.listdir('../input/train_simplified/')\nclass_dict = {x[:-4].replace(\" \", \"_\"):i for i, x in enumerate(class_files)}\nreverse_dict = {v: k for k, v in class_dict.items()}\n\nclass_fraction = 340/340                     ## \nimg_rows, img_cols = 32, 32               ##\nims_per_class = 1500                   ##\n\nnum_classes = int(len(class_files)*class_fraction)\nnum_rows = ims_per_class*5//4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ce1101eea3b2a0787c1dbe6b675e2a481802f67","_kg_hide-output":false,"scrolled":true},"cell_type":"code","source":"# more efficient conversion function\ndef draw_it(strokes):\n    image = Image.new(\"P\", (256,256), color=255)\n    image_draw = ImageDraw.Draw(image)\n    for stroke in ast.literal_eval(strokes):\n        for i in range(len(stroke[0])-1):\n            image_draw.line([stroke[0][i], \n                             stroke[1][i],\n                             stroke[0][i+1], \n                             stroke[1][i+1]],\n                            fill=0, width=5)\n    image = image.resize((img_rows, img_cols))\n    return np.array(image)/255.\n\ndef dothething(chunk):    \n    chunk = chunk[chunk.recognized == True]\n    chunk['word'] = chunk.word.str.replace(\" \", \"_\")\n    imagebag = bag.from_sequence(chunk.drawing.values).map(draw_it) \n    chunkarray = np.array(imagebag.compute())  # PARALLELIZE\n    chunkarray = np.reshape(chunkarray, (chunk.shape[0], -1))\n    labelarray = chunk.word.map(class_dict).values\n    labelarray = np.expand_dims(labelarray, 1)\n    chunkarray = np.concatenate((labelarray, chunkarray), axis=1)\n    return chunkarray","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"27179356f74c117f24d3e2d074f304740bf2cd04","_kg_hide-output":true,"scrolled":false},"cell_type":"code","source":"# slower with chunks but easier on memory\ntrain_grand = []\n# train_grand = np.empty((num_classes, ims_per_class, img_rows*img_cols+1))\nclass_paths = glob('../input/train_simplified/*.csv')\nfor i,c in enumerate(tqdm(class_paths[0: num_classes])):\n    classims = []\n    reader = pd.read_csv(c, usecols=['word', 'drawing', 'recognized'], nrows=num_rows, chunksize=600)\n    for i,chunk in enumerate(reader):    \n        chunkarray = dothething(chunk)\n        classims.append(chunkarray)\n#     train_grand[i] = (np.concatenate(classims)[0:ims_per_class])\n    classims = classims[0:ims_per_class]\n    classims = np.concatenate(classims)\n    train_grand.append(classims)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a8b741677292796da32579272f24c84f488bc469","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import gc\ndel classims\ndel chunkarray\ndel chunk\ngc.collect()\n\ntrain_grand = np.concatenate(train_grand) # expensive operation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7fa29f67ab578e7ae483ba3cec9f901b6c867e72","_kg_hide-input":false,"_kg_hide-output":true},"cell_type":"code","source":"# memory-friendly alternative to train_test_split\nvalfrac = 0.1\ncutpt = int((1-valfrac) * train_grand.shape[0])\n\nnp.random.shuffle(train_grand)\nx_train = train_grand[0:cutpt, 1:]\ny_train = train_grand[0:cutpt, 0]\nx_val = train_grand[cutpt: , 1:]\ny_val = train_grand[cutpt: , 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"940280a709811b2c6a06c1673d83023d83eab10b"},"cell_type":"code","source":"x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\nx_val = x_val.reshape(x_val.shape[0], img_rows, img_cols, 1)\ny_train_onehot = keras.utils.to_categorical(y_train, num_classes)\ny_val_onehot = keras.utils.to_categorical(y_val, num_classes)\n\nprint(x_train.shape, \"\\n\",\n      y_train_onehot.shape, \"\\n\",\n      x_val.shape, \"\\n\",\n      y_val_onehot.shape)\n\ndel train_grand","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"24ef673ac1b339a6449a2e20ecf946b40dbc0ca3"},"cell_type":"markdown","source":"Here's the architecture for the CNN. It's fairly simple compared to what else you can do."},{"metadata":{"trusted":true,"_uuid":"002b62eec9f0b5c139c4c08f82476433ccc11026"},"cell_type":"code","source":"model = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu', input_shape=(img_rows, img_cols, 1)))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.2))\nmodel.add(Flatten())\nmodel.add(Dense(680, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6464e84f6f0c7303110db9e26c74a5c90a0a9ab9","_kg_hide-output":true,"scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"# commented out for commit\n# model.compile(loss='categorical_crossentropy',\n#               optimizer='adam',\n#               metrics=['accuracy'])\n\n# model.fit(x_train, y_train_onehot,\n#           batch_size = 32,\n#           epochs = 9,\n#           validation_data=(x_val, y_val_onehot),\n#           verbose = 2)\n\n# score = model.evaluate(x_val, y_val_onehot, verbose = 1)\n# print(\"\\nAccuracy {}\".format(score[1]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4125359749cd237de207eca0ab62074431df8323"},"cell_type":"markdown","source":"## Predicting on the Test data\nThe CNN does OK on the validation data, even with a basic model and limited training data. Let's generate predictions on the test set and submit."},{"metadata":{"trusted":true,"_uuid":"2828f46cdf053261a1435e6eca1a4aea55195273","_kg_hide-input":false,"_kg_hide-output":true},"cell_type":"code","source":"# refit on all data\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu', input_shape=(img_rows, img_cols, 1)))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.2))\nmodel.add(Flatten())\nmodel.add(Dense(680, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\nmodel.fit(np.vstack((x_train, x_val)),\n          np.vstack((y_train_onehot, y_val_onehot)),\n          # no validation\n          batch_size = 32,\n          epochs = 9,\n          verbose = 2)\n\ndel x_train\n\n#%% get test set\nttvlist = []\nreader = pd.read_csv('../input/test_simplified.csv', index_col=['key_id'],\n    chunksize=1024)\nfor chunk in tqdm(reader):\n    imagebag = bag.from_sequence(chunk.drawing.values).map(draw_it)\n    testarray = np.array(imagebag.compute())\n    testarray = np.reshape(testarray, (testarray.shape[0], img_rows, img_cols, 1))\n    testpreds = model.predict(testarray, verbose=0)\n    ttvs = np.argsort(-testpreds)[:, 0:3]  # top 3\n    ttvlist.append(ttvs)\n    \nttvarray = np.concatenate(ttvlist)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5c650c6f9a0ec6f99042e6132ffc4a25b2b2f0b3","_kg_hide-input":false,"_kg_hide-output":true},"cell_type":"code","source":"preds_df = pd.DataFrame({'first': ttvarray[:,0], 'second': ttvarray[:,1], 'third': ttvarray[:,2]})\npreds_df = preds_df.replace(reverse_dict)\npreds_df['words'] = preds_df['first'] + \" \" + preds_df['second'] + \" \" + preds_df['third']\n\nsub = pd.read_csv('../input/sample_submission.csv', index_col=['key_id'])\nsub['word'] = preds_df.words.values\nsub.to_csv('subcnn_small.csv')\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"494ef170cb02e2425e82a897a0ca6c794c676b5a"},"cell_type":"markdown","source":" A full version with 6000 images per class at 28x28 gets just under 0.60 on the public LB.  "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}